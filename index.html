<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
  body {
    background-color: #f5f9ff;
  }

  .mathjax-mobile, .mathml-non-mobile { display: none; }
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

  .content-margin-container {
    display: flex;
    width: 100%;
    justify-content: left;
    align-items: center;
  }
  .main-content-block {
    width: 70%;
    max-width: 1100px;
    background-color: #fff;
    border-left: 1px solid #DDD;
    border-right: 1px solid #DDD;
    padding: 8px 8px 8px 8px;
    font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
  }
  .margin-left-block {
      font-size: 14px;
      width: 15%;
      max-width: 130px;
      position: relative;
      margin-left: 10px;
      text-align: left;
      font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      padding: 5px;
  }
  .margin-right-block {
      font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      font-size: 14px;
      width: 25%;
      max-width: 256px;
      position: relative;
      text-align: left;
      padding: 10px;
  }

  img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: auto;
  }
  .my-video {
      max-width: 100%;
      height: auto;
      display: block;
      margin: auto;
  }

  .vid-mobile, .vid-non-mobile { display: none; }
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

  a:link,a:visited {
    color: #0e7862;
    text-decoration: none;
  }
  a:hover {
    color: #24b597;
  }

  h1 {
    font-size: 18px;
    margin-top: 4px;
    margin-bottom: 10px;
  }

  table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
    width: 70%;
    max-width: calc(100% - 290px);
  }
  table td, table td * {
      vertical-align: middle;
      position: relative;
  }
  table.paper-code-tab {
      flex-shrink: 0;
      margin-left: 8px;
      margin-top: 8px;
      padding: 0px 0px 0px 8px;
      width: 290px;
      height: 150px;
  }

  .layered-paper {
    box-shadow:
          0px 0px 1px 1px rgba(0,0,0,0.35),
          5px 5px 0 0px #fff,
          5px 5px 1px 1px rgba(0,0,0,0.35),
          10px 10px 0 0px #fff,
          10px 10px 1px 1px rgba(0,0,0,0.35);
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

  div.hypothesis {
    width: 80%;
    background-color: #EEE;
    border: 1px solid black;
    border-radius: 10px;
    -moz-border-radius: 10px;
    -webkit-border-radius: 10px;
    font-family: Courier;
    font-size: 16px;
    text-align: center;
    margin: auto;
    padding: 12px 16px 12px 16px;
  }

  div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
    height: 200px;
  }

  .fade-in-inline {
    position: absolute;
    text-align: center;
    margin: auto;
    -webkit-mask-image: linear-gradient(to right,
                                      transparent 0%,
                                      transparent 40%,
                                      black 50%,
                                      black 90%,
                                      transparent 100%);
    mask-image: linear-gradient(to right,
                                transparent 0%,
                                transparent 40%,
                                black 50%,
                                black 90%,
                                transparent 100%);
    -webkit-mask-size: 8000% 100%;
    mask-size: 8000% 100%;
    animation-name: sweepMask;
    animation-duration: 4s;
    animation-iteration-count: infinite;
    animation-timing-function: linear;
    animation-delay: -1s;
  }

  .fade-in2-inline {
      animation-delay: 1s;
  }

  .inline-div {
      position: relative;
      display: inline-block;
      vertical-align: top;
      width: 50px;
  }

</style>

    <title>The Platonic Representation Hypothesis</title>
      <meta property="og:title" content="The Platonic Representation Hypothesis" />
      <meta charset="UTF-8">
  </head>

  <body>

    <div class="content-margin-container">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <table class="header" align=left>
                <tr>
                  <td colspan=4>
                    <span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Judging the Judges: Probing and Causal Debiasing of LLM-as-Jury Evaluators</span>
                  </td>
                </tr>
                <tr>
                    <td align=left>
                        <span style="font-size:17px"><a href="your_website">Audrey Wei</a></span>
                    </td>
                    <td align=left>
      
                    </td>
                <tr>
                  <td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960 Fall 2025, MIT</span></td>
                </tr>
            </table>
          </div>
          <div class="margin-right-block">
          </div>
    </div>

    <div class="content-margin-container" id="intro">
        <div class="margin-left-block">
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#does_x_do_y">Background &amp; Setup</a><br><br>
              <a href="#methods">Methods &amp; Experiments</a><br><br>
              <a href="#results">Results &amp; Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
        </div>
        <div class="main-content-block">

        </div>
        <div class="margin-right-block">

        </div>
    </div>

    <div class="content-margin-container" id="intro">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <h1>Introduction</h1>
            Modern deep learning systems increasingly rely on <i>LLM-as-judge</i> to evaluate generated text. Instead of asking humans to rate every answer, we delegate evaluation to another language model that scores or ranks candidate responses. This is useful because it is scalable, cheap, and easy to integrate into training pipelines and benchmarks. However, it raises a central question: what exactly are these judges rewarding, and how do those preferences arise inside the model?<br><br>

            MT-Bench shows that LLM judges can approximate human preferences on average, but they also exhibit systematic biases toward superficial features such as answer position and verbosity <a href="#ref_2">[2]</a>. Recent work finds that these “LLM-as-a-judge” systems can be skewed by politeness, safety disclaimers, and other surface cues, and that debiasing is non-trivial <a href="#ref_3">[3]</a>, <a href="#ref_4">[4]</a>, <a href="#ref_5">[5]</a>. At the same time, probing studies suggest that many linguistic properties are linearly decodable from intermediate representations, while more abstract notions such as semantics or truth are harder to isolate <a href="#ref_6">[6]</a>.<br><br>


              Our hypothesis is that stylistic biases (e.g., length, redundancy, sentiment, position) are encoded as low-dimensional directions that are easy to decode and causally intervene on, whereas correctness is represented in a more diffuse, non-linear way. As a result, we should be able to reliably remove some biases, such as position, via simple projections without hurting correctness, while other “biases” such as length are entangled with useful signal.

            <br>

            In this project we study a more complex setting in which multiple open-source models act as judges on MT-Bench, forming an <i>LLM-as-jury</i>. We measure a broad suite of stylistic and semantic features (length, sentiment, redundancy, hedging, safety language, formality, etc.) and explore what biases arise when LLMs are used as judges, and how do they relate to each other, how we can interpret these biases as directions in representation space and causally intervene on them, and at which layers of the network are correctness and bias signals encoded.

            To answer these questions, we combine three parts: (1) a multi-judge evaluation protocol on MT-Bench; (2) a causal intervention judge that projects out gradient-defined bias directions in embedding space; and (3) sparse probing of hidden states across layers. Together, these results show how LLM judges make decisions and where their biases live in the network.
        </div>
        <div class="margin-right-block">
            
        </div>
    </div>

    <div class="content-margin-container" id="does_x_do_y">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
          <h1>Background and problem setup</h1>
          MT-Bench evaluates LLMs via multi-turn questions and asks a judge model to decide which of two answers is better <a href="#ref_2">[2]</a>. Follow-up work has documented position bias, verbosity bias, and other systematic errors in LLM-based evaluation, and begun to analyze them with causal interventions and probing <a href="#ref_3">[3]</a>, <a href="#ref_4">[4]</a>, <a href="#ref_5">[5]</a>. However, these studies typically focus on a single judge model and a narrow set of biases, leaving open how multiple biases interact and where they are encoded.<br><br>

          We view a judge as assigning a scalar score to each prompt–answer pair, which we operationalize as the log-likelihood of the answer conditioned on the conversation context.
          Intuitively, this score mixes (i) a component related to answer quality and alignment with human preferences, and (ii) components driven by surface-level features such as length, redundancy, formality, and sentiment. Our goal is to disentangle these parts.<br><br>

          We adopt a geometric view: each bias corresponds to a direction in representation space. If <i>h</i> denotes an input embedding and <i>v<sub>b</sub></i> a learned bias direction, we define a <b>debiased representation</b> by subtracting the projection of <i>h</i> onto <i>v<sub>b</sub></i>. Recomputing log-likelihoods from this debiased embedding allows us to estimate how much of the judge’s behavior is causally attributable to that bias direction. We then compare these causal effects with what we can decode from hidden states using sparse probes.
        </div>
        <div class="margin-right-block" style="transform: translate(0%, -100%);">
 
        </div>
    </div>

    <div class="content-margin-container" id="methods">
  <div class="margin-left-block"></div>

  <div class="main-content-block">
    <h1>Methods and experiments</h1>

    Our experimental pipeline has three main components. First, we construct an LLM-as-Jury setup in which four open-source models (LLaMA-3.1-8B, Qwen-2.5-7B, Mistral-7B, and Phi-3-3.8B) independently judge every MT-Bench answer pair using a unified scoring prompt. Their outputs populate a shared table that we later aggregate into several meta-judge strategies. Second, we compute a large suite of stylistic and semantic bias features for every answer, enabling descriptive and correlation-based analyses. Finally, we perform two deeper diagnostic experiments: (1) causal interventions, where we estimate gradient-based directions encoding particular biases and project them out of the embedding space; and (2) sparse probing across all transformer layers to examine where correctness and bias signals reside.

    <h2>Building the evaluation dataset</h2>

    We start from the MT-Bench human-judgment release. From the provided JSONL files we extract full prompt–answer conversations for each question_id, serialize them into a single prompt-context string, and record the final assistant message from each model as its answer. We store these in a table (`mt_bench_for_manual_llm_judges_human.csv`) containing prompt_context, answer_a, answer_b, model identities, and the raw human winner (A/B/tie). This table forms the basis of all subsequent judgment and analysis steps.

    <h2>Local multi-judge scoring with Ollama</h2>

    To populate judgments for all answer pairs, each of the four models is run locally through Ollama. For each row in the dataset, we send:

<pre>[PROMPT CONTEXT]
{prompt_context}

[ANSWER A]
{answer_a}

[ANSWER B]
{answer_b}</pre>

    together with a shared system prompt instructing the model to output integer scores (1–10) for A and B and a categorical winner (“A” or “B”), with ties disallowed.

    Model outputs frequently include code fences or extraneous text, so we implement a robust `extract_json_object` routine that strips formatting and extracts the substring between the first '{' and final '}'. If JSON still fails to parse, we fallback to a neutral placeholder (5–5, winner=A) to preserve pipeline stability.

    Each model is run independently. For every prefix (llama, qwen, mistral, phi) we create `{prefix}_score_a`, `{prefix}_score_b`, and `{prefix}_winner`, skipping any row already filled from a prior run. We periodically write updated CSV and Parquet files (`mt_bench_with_phi_llama_mistral_qwen.*`).

    <h2>Meta-judges: single-model vs ensemble scoring</h2>

    After all judge outputs are collected, we expand each A/B comparison into two answer-level examples: the A answer at position=0 and the B answer at position=1. We convert the human winner into a binary correctness label for each answer (ties give 0 to both). On top of this expanded dataset we define four judge strategies:

    <ul>
      <li>Single-LLM: use only the LLaMA-3.1-8B scores and winner.</li>
      <li>Unweighted average: average the four models’ numeric scores and select the higher-scoring answer.</li>
      <li>Majority vote: take the mode of the four per-model winners.</li>
      <li>Parameter-weighted average: compute a weighted mean of scores using model size as a proxy for reliability (e.g., 8.0 for LLaMA, 7.0 for Qwen and Mistral, 3.8 for Phi).</li>
    </ul>

    Each strategy yields its own predicted winner for every pair. These expanded rows—with human labels, judge predictions, and all bias features—are saved as `examples_with_biases.csv`. We then assess the agreement of each judge with human preference and analyze accuracy as a function of bias intensity.

    <h2>Bias feature extraction</h2>

    For every answer we compute a dictionary of features using `compute_bias_features`. These include:

    <ul>
      <li>Position and length: position∈{0,1}, token count, character count, verbosity (alias of tokens).</li>
      <li>Digits and symbols: counts of digit characters and a fixed symbol set.</li>
      <li>Sentiment: a lightweight lexicon-based score normalized by token length.</li>
      <li>Hedging and safety: densities based on curated phrase lists (“I think…”, “as an AI…”, etc.).</li>
      <li>Formality: derived from pronoun density and contraction usage.</li>
      <li>Redundancy and novelty: Jaccard overlaps of unigrams and bigrams between the prompt and answer, with novelty defined as 1 – redundancy.</li>
    </ul>

    These features support correlation studies, PCA visualizations, and conditional accuracy analyses used later in the Results section.

    <h2>Causal intervention judge (gradient-based debiasing)</h2>

    To isolate how much a specific bias direction influences model scoring, we construct a causal intervention pipeline using Qwen-2.5-1.5B-Instruct. The workflow is:

    <ol>
      <li>Compute original log-likelihoods for each prompt–answer pair using a unified input format (`"Prompt:\n... Answer:\n..."`).</li>

      <li>Estimate a bias direction for a chosen scalar feature (length_tokens, sentiment, position, redundancy):
        <ul>
          <li>Select examples from the high and low ends of the bias distribution (top/bottom deciles, up to 4 per group).</li>
          <li>For each example, compute gradients of the sequence log-likelihood with respect to token embeddings.</li>
          <li>Form attribution vectors via elementwise `embeds * grads` and mean-pool over tokens.</li>
          <li>Average the high-bias and low-bias pooled vectors and take their difference; normalize to obtain a unit direction \(v_b\).</li>
        </ul>
      </li>

      <li>Debias each evaluation example by projecting out the direction \(v_b\) from its embedding sequence:
<pre>E_debias = E - (E · v_b) v_b</pre>

          then recompute the log-likelihood. We store Δ = (debiased − original) for every example, along with metadata.</li>
    </ol>

    We repeat this for each bias dimension, producing CSV logs of Δ distributions that quantify which biases have large, neutral, or harmful causal influence.

    <h2>Sparse probing across layers</h2>

    To localize bias and correctness signals inside the network, we train layerwise L1-regularized logistic probes:

    <ol>
      <li>For every example, run Qwen-2.5-1.5B with `output_hidden_states=True` and mean-pool each layer’s hidden state over unmasked tokens, producing one D-dimensional vector per layer.</li>
      <li>Construct binary targets: correctness, position, sentiment, length-threshold indicators, redundancy, novelty, hedging, safety, formality, pronoun density, overlap metrics, and others.</li>
      <li>Train a separate L1-penalized logistic regression probe per layer and per target (80/20 stratified split). Record the probe’s classification accuracy and sparsity (fraction of zero weights).</li>
    </ol>

    These probe profiles show how easily each feature can be linearly decoded from each layer, and their layer-wise correlations reveal how different biases share or diverge in representational structure.

    <br><br>
    Together, the causal intervention logs, meta-judge comparisons, and probe results give a detailed picture of how LLM judges encode surface properties, which signals dominate their scoring behavior, and how these preferences can be manipulated or mitigated.
  </div>

  <div class="margin-right-block">
    We visualize meta-judge accuracy, PCA structure, Δ-log-likelihood distributions, and probe accuracy curves to link surface biases, internal representations, and causal influence.
  </div>
</div>


    <div class="content-margin-container" id="results">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <h1>Results &amp; Discussion</h1>

            <h2>3.1 What biases exist in the LLM-as-jury?</h2>

            <div style="text-align:center;">
              <img src="./1ss.png" width="400px" />
              <div style="font-size:14px; color:#555;">
                <b>Figure 1.</b> Correlation matrix of extracted bias features across all answers.
              </div>
            </div>

            <p>
              The correlation matrix shows that several bias features naturally cluster. Redundancy, unigram overlap,
              and bigram overlap are almost perfectly correlated, forming a single “prompt-copying” axis with novelty as its inverse.
              Length-related features (tokens, characters, digits, symbols) form another coherent group, indicating that verbosity behaves
              as a consistent stylistic mode. In contrast, position, sentiment, hedging, safety language, formality, and pronoun density
              have only weak correlations with other features, suggesting they represent distinct bias mechanisms worth analyzing separately.
            </p>

            <div style="text-align:center;">
              <img src="./2ss.png" width="400px" />
              <div style="font-size:14px; color:#555;">
                <b>Figure 2.</b> PCA loadings of bias features, showing how each bias contributes to the top two principal components.
              </div>
            </div>

            <p>
              PCA loadings confirm this structure: one principal component is dominated by length/verbosity, while another is dominated
              by redundancy and overlap. Stylistic indicators such as pronoun density, sentiment, hedging, safety, and formality occupy
              a separate, weaker axis of variation. Importantly, none of these components aligns strongly with correctness, anticipating
              our later result that stylistic biases are largely orthogonal to answer quality.
            </p>

            <p>
              We find that all features are only weakly correlated with human correctness; even the largest effects
              (length, redundancy, novelty, unigram overlap) are on the order of 0.05–0.06. Hedging, safety language, formality, and
              pronoun usage are even less predictive. This supports our hypothesis that surface style is not a reliable signal of quality
              and should be treated as bias rather than a useful evaluation feature.
            </p>

            <div style="text-align:center;">
              <img src="./5_ss.png" width="600px" />
              <div style="font-size:14px; color:#555;">
                <b>Figure 4.</b> PCA projection of bias features, colored by answer position (A vs. B).
              </div>
            </div>

            <p>
              Surprisingly, answers from slots A and B separate cleanly in the PCA space, meaning that the two positions induce distinct
              stylistic regimes (e.g., in length, redundancy, and overlap). Because PCA is unsupervised, this indicates that position is
              not just a dummy index but changes how answers are written, helping explain why LLM judges often show strong position bias.
            </p>

            <h2>3.2 Causal interventions on bias directions</h2>

            <div style="text-align:center;">
              <img src="./7ss.png" width="600px" />
              <div style="font-size:14px; color:#555;">
                <b>Figure 5.</b> Distribution of Δ log-likelihood (debiased − original) across three bias directions:
                <i>length_tokens</i>, <i>position</i>, and <i>redundancy</i>. The dashed line marks zero change.
              </div>
            </div>

            <p>
              Projecting out the <i>position</i> direction consistently increases log-likelihood for human-preferred answers,
              indicating that position bias actively harms the judge’s performance and can be removed without sacrificing useful signal.
              Intervening on <i>redundancy</i> yields smaller but mostly positive shifts, suggesting a moderate, recoverable bias toward
              copying the prompt. In contrast, removing the <i>length_tokens</i> direction often <i>reduces</i> log-likelihood, implying that
              what looks like a verbosity “bias” actually carries information the model relies on to score answers.
            </p>

            <div style="text-align:center;">
              <img src="./6ss_box.png" width="600px" />
              <div style="font-size:14px; color:#555;">
                <b>Figure 6.</b> Effect of debiasing on Δ log-likelihood, split by human correctness and bias type.
              </div>
            </div>

            <p>
              When we split Δ log-likelihood by human correctness, the picture sharpens. Removing <i>position</i> bias improves scores for both
              correct and incorrect answers, but the gains are largest on pairs that were initially misjudged, directly supporting our
              hypothesis that position is a harmful bias. For <i>length_tokens</i>, debiasing consistently harms both groups, confirming that
              length is entangled with genuinely useful cues. <i>Redundancy</i> lies in between: debiasing helps on many misjudged examples but has
              smaller effects elsewhere. Overall, these interventions show that not all “biases” are equal—some are safe to remove, while
              others behave more like compressed heuristics for quality.
            </p>

            <h2>3.3 Where do correctness and biases live in the network?</h2>

            <figure>
                <img src="./8ss.png" alt="Probe accuracy across layers for key targets" style="width:90%;">
                <figcaption><strong>Figure 7.</strong> Probe accuracy across layers for correctness and major bias features.</figcaption>
            </figure>

            <p>
              Sparse probes reveal a striking gap between correctness and stylistic biases. Features such as length, position, redundancy,
              sentiment, and formality become highly decodable (≥0.9 accuracy) by very early layers (around layers 5–10) and stay that way
              throughout the network. Correctness, by contrast, never becomes linearly decodable and hovers near 0.55 across all layers.
              This directly supports our hypothesis: biases align with simple low-dimensional directions, whereas correctness does not.
            </p>

            <figure>
                <img src="./9ss.png" alt="Correlation between targets' accuracy profiles over layers" style="width:90%;">
                <figcaption><strong>Figure 8.</strong> Correlation heatmap showing similarity between targets’ layer-wise probe accuracy curves.</figcaption>
            </figure>

            <p>
              The correlation heatmap shows that most bias features share very similar layer-wise decodability profiles, forming a tight
              cluster that we can think of as a “style manifold.” Correctness is the clear outlier: its probe accuracy profile is only
              weakly or negatively correlated with those of other targets. Combined with the high sparsity of successful bias probes,
              this suggests that stylistic preferences are encoded in a compact, structured subspace, while correctness is distributed and
              not directly tied to any single set of dimensions. In other words, LLM judges make decisions in a space where bias is
              geometrically simple but correctness is geometrically messy.
            </p>

            <p>
              Taken together, the descriptive statistics, causal interventions, and probing results support our main hypothesis.
              LLM-as-jury systems encode many stylistic biases as clear directions that we can decode and manipulate. Some of these,
              like position, are genuinely harmful and can be removed to bring the judge closer to human preferences. Others, like length,
              function more like heuristics that correlate weakly with quality and are risky to zero out. Correctness itself never becomes
              an easily accessible feature, highlighting a fundamental mismatch between what is easy for the model to represent and what
              we actually want it to reward.
            </p>
        </div>
        <div class="margin-right-block">
        </div>
    </div>

    <div class="content-margin-container" id="conclusion">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <h1>Conclusion</h1>
            Our analysis suggests that LLM-based judges are not monolithic “oracles,” but systems whose preferences can be decomposed into
            interpretable bias dimensions and manipulated via simple linear interventions. Multi-model ensembles (“LLM-as-jury”) smooth out
            some certain biases of individual judges, especially extreme position and verbosity preferences, but they do not fully
            eliminate systematic tendencies toward redundancy, politeness, and prompt-copying.<br><br>

            Causal interventions show that some biases, notably position, can be projected out with clear benefits to alignment with human
            judgments, while others such as length encode genuinely useful cues and are dangerous to suppress completely. Probing results
            complement this picture: stylistic and structural features are highly linearly decodable from early layers, forming a compact
            “style manifold,” whereas correctness remains diffuse and never becomes an easily accessible feature. This structural mismatch
            explains why naive reliance on judge scores risks baking shallow biases into training loops, leaderboards, and deployment
            decisions.<br><br>

            There are important limitations. Our interventions assume approximate linearity of bias directions, and highly non-linear
            interactions may go undetected. Probes measure decodability, not causality, and MT-Bench covers a limited range of tasks and
            domains. We also focus on relatively small open-source judges; larger proprietary models may exhibit different, subtler bias
            structures. Despite these caveats, the tools we develop—multi-bias feature extraction, causal projection, and layerwise sparse
            probing—provide a reusable framework for stress-testing future LLM-as-judge systems and for designing debiased evaluators that
            are more faithful to human intent.
        </div>
        <div class="margin-right-block">
          Future directions: train a dedicated, debiased judge using projected representations; extend to other modalities (e.g., vision–language); and explore counterfactual data augmentation to further break spurious style–quality correlations.
        </div>
    </div>

    <div class="content-margin-container" id="citations">
        <div class="margin-left-block">
        </div>
        <div class="main-content-block">
            <div class='citation' id="references" style="height:auto"><br>
              <span style="font-size:16px">References:</span><br><br>
              <a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
              <a id="ref_2"></a>[2] <a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench</a>, Zheng et al., 2023<br><br>
              <a id="ref_3"></a>[3] <a href="https://arxiv.org/abs/2406.07791">Biases in Large Language Model Evaluations: A Causal and Probing Study</a>, Crabbé et al., 2024<br><br>
              <a id="ref_4"></a>[4] <a href="https://arxiv.org/abs/2410.02736">Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge</a>, Ye et al., 2024<br><br>
              <a id="ref_5"></a>[5] <a href="https://arxiv.org/abs/2406.13831">Mitigating the Bias of Large Language Model Evaluation</a>, Zhou et al., 2024<br><br>
              <a id="ref_6"></a>[6] <a href="https://arxiv.org/abs/2104.08197">Probing Artificial Neural Networks: Insights from Neuroscience</a>, Ivanova et al., 2021<br><br>
            </div>
        </div>
        <div class="margin-right-block">
        </div>
    </div>

  </body>

</html>
